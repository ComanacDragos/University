\section{Introduction}

According to the World Health Organization \cite{worldReportOnVision}, the number of people suffering from some moderate to severe form of distance vision impairment or blindness due to cataract or uncorrected refractive error is around 200 million, out of the total of 2.2 billion people worldwide that are estimated to have problems with their vision. This represents a significant segment of the population that has trouble performing daily tasks. These troubles can be alleviated using assistive technologies that can help persons with disabilities maintain or enhance their capabilities. Given the number of people that suffer from some form of visual impairment and the fact that computers can substitute visual functionalities, computer vision has the potential to play the main part of assistive technology for visually impaired persons (VIP), such that it helps the user to better understand the surrounding environment when performing different kinds of tasks.
%Computers can extract and interpret meaningful information from digital images or videos using Computer Vision (CV), which is a multidisciplinary field of artificial intelligence (AI) and deep learning (DL). It can also be regarded as a way of replicating the functions of the eye and the human visual cortex, or the area of the brain that processes visual information. So far, the functions of the eye are replicated, even exceeded, quite well by modern cameras. The functions of the visual cortex prove to be more difficult to replicate, but modern AI models can perform well on very specific tasks, sometimes rivaling human image recognition capabilities.

%Traditionally, running a complex enough deep learning model required so many resources that it could only run on large and expensive equipment. But, as technology advances, as predicted by Moore's law, powerful devices become more compact and are accessible to more people. Nowadays, phones have the needed hardware, such as integrated high-resolution cameras and fast CPUs, to use AI models to extract and interpret information about the surrounding environment. This means that computer vision can be integrated into daily activities.
%Given the number of people that suffer from some form of visual impairment and the fact that computers can substitute visual functionalities, computer vision has the potential to play the main part of assistive technology for visually impaired persons (VIP), such that it helps the user to better understand the surrounding environment when performing different kinds of tasks. %It is worth noting that this kind of technology can also prevent accidents. Also, a bonus is that anyone can have a phone, which is relatively cheaper and easier to use, compared to other specialized devices that might not even be available in all countries.

One of the fundamental human needs is mobility and it can be achieved through public transport. This way of traveling is especially important to the VIP, since they can't drive. Therefore, the main ways a VIP can travel is by public transport, ridesharing, or taxi, but they experience many difficulties on their journeys, often experiencing social exclusion because they are limited in their choices of public transport \cite{vipExperienceOnPT}. 
        
Given that most mobility solutions don't provide adequate accessibility facilities, we developed \textbf{a mobile assistive technology solution} for VIP that provides spatial information by using a \textbf{real-time object detection model} inspired by You Only Look Once \cite{yolo}. More specifically, the user can get information about buses or cars from the auditory information provided by the mobile application. All this information is extracted from the bounding boxes predicted by the object detection model. Also, information about the license plates can be provided to help the user to identify the vehicle.


The main contributions of this work are two-fold: first we proposed a pipeline to improve VIP access to public transport. We start by taking live images and output the bounding boxes in the form of audio, on the mobile device. Secondly, we implemented from scratch and trained using fine-tuning an object detector inspired by YOLOv2 \cite{yolov2}. For this process we performed in-depth data analysis and we enhanced the train datasets with the locations of license plates using an off-the-shelf, state of the art object detector \cite{license_plate_enhancer}. Finally, we experimented with several augmentation techniques. To this end, we obtain a mean average precision of 70.03\%, and for the bus class, we obtain an average precision of 90.01\%, for the car class 64.04\% and for the vehicle registration plate 56.68\%, with a speed of around 5 FPS on a mobile device.

%Our main achievements are that we implemented and trained the object detection system, and that we composed from scratch the pipeline that takes live images and outputs the bounding boxes in the form of audio, only on the mobile device. On the test set, we obtain, a mean average precision of 70.03\%, and for the bus class, we obtain an average precision of 90.01\%, for the car class 64.04\% and for the vehicle registration plate 56.68\%, with a speed of around 5 FPS on a mobile device.

In what follows, we first describe the current approaches in terms of public transport accessibility and object detection in Section \ref{soa}. Afterwards, in Section \ref{object_detector} we present the implementation details of our approach by detailing our object detector. Then, we discuss the performance of our object detector and how it is compared to other object detection systems in Section \ref{experimental_results}. Finally, we briefly present how we deploy our object detector on a mobile phone in Section \ref{deployment}. 